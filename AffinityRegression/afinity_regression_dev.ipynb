{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.nature.com/articles/nbt.3343\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from Bio import SeqIO\n",
    "\n",
    "file_2 = 'dropbox_files/Full_predictableRRM_pearson_gt3_trainset216_test24.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ###Set 0\n",
    "\n",
    "# ##Train:\n",
    "\n",
    "# RNCMPT00004 RNCMPT00005 RNCMPT00008 RNCMPT00010 RNCMPT00012 RNCMPT00013 RNCMPT00014 RNCMPT00017 RNCMPT00021 RNCMPT00022 RNCMPT00023 RNCMPT00024 RNCMPT00025 RNCMPT00027 RNCMPT00028 RNCMPT00029 RNCMPT00037 RNCMPT00040 RNCMPT00043 RNCMPT00045 RNCMPT00049 RNCMPT00050 RNCMPT00051 RNCMPT00054 RNCMPT00056 RNCMPT00057 RNCMPT00058 RNCMPT00059 RNCMPT00061 RNCMPT00064 RNCMPT00066 RNCMPT00067 RNCMPT00068 RNCMPT00069 RNCMPT00071 RNCMPT00072 RNCMPT00076 RNCMPT00079 RNCMPT00080 RNCMPT00081 RNCMPT00088 RNCMPT00095 RNCMPT00099 RNCMPT00108 RNCMPT00114 RNCMPT00120 RNCMPT00121 RNCMPT00122 RNCMPT00123 RNCMPT00126 RNCMPT00127 RNCMPT00132 RNCMPT00133 RNCMPT00134 RNCMPT00136 RNCMPT00138 RNCMPT00139 RNCMPT00140 RNCMPT00141 RNCMPT00143 RNCMPT00144 RNCMPT00145 RNCMPT00146 RNCMPT00150 RNCMPT00151 RNCMPT00152 RNCMPT00153 RNCMPT00155 RNCMPT00156 RNCMPT00157 RNCMPT00158 RNCMPT00159 RNCMPT00160 RNCMPT00165 RNCMPT00166 RNCMPT00167 RNCMPT00168 RNCMPT00171 RNCMPT00176 RNCMPT00177 RNCMPT00178 RNCMPT00179 RNCMPT00180 RNCMPT00181 RNCMPT00182 RNCMPT00183 RNCMPT00184 RNCMPT00187 RNCMPT00199 RNCMPT00200 RNCMPT00202 RNCMPT00203 RNCMPT00205 RNCMPT00206 RNCMPT00209 RNCMPT00216 RNCMPT00217 RNCMPT00218 RNCMPT00220 RNCMPT00223 RNCMPT00226 RNCMPT00228 RNCMPT00229 RNCMPT00230 RNCMPT00232 RNCMPT00234 RNCMPT00235 RNCMPT00236 RNCMPT00237 RNCMPT00238 RNCMPT00241 RNCMPT00245 RNCMPT00248 RNCMPT00249 RNCMPT00251 RNCMPT00252 RNCMPT00253 RNCMPT00254 RNCMPT00255 RNCMPT00256 RNCMPT00257 RNCMPT00258 RNCMPT00262 RNCMPT00263 RNCMPT00268 RNCMPT00270 RNCMPT00272 RNCMPT00279 RNCMPT00280 RNCMPT00281 RNCMPT00282 RNCMPT00283 RNCMPT00284 RNCMPT00285 RNCMPT00288 RNCMPT00289 RNCMPT00291 RNCMPT00306 RNCMPT00311 RNCMPT00323 RNCMPT00325 RNCMPT00326 RNCMPT00331 RNCMPT00340 RNCMPT00343 RNCMPT00344 RNCMPT00345 RNCMPT00346 RNCMPT00349 RNCMPT00364 RNCMPT00366 RNCMPT00368 RNCMPT00370 RNCMPT00372 RNCMPT00380 RNCMPT00382 RNCMPT00392 RNCMPT00394 RNCMPT00396 RNCMPT00399 RNCMPT00403 RNCMPT00413 RNCMPT00414 RNCMPT00416 RNCMPT00418 RNCMPT00419 RNCMPT00421 RNCMPT00422 RNCMPT00425 RNCMPT00430 RNCMPT00431 RNCMPT00433 RNCMPT00434 RNCMPT00461 RNCMPT00462 RNCMPT00464 RNCMPT00466 RNCMPT00468 RNCMPT00471 RNCMPT00475 RNCMPT00476 RNCMPT00503 RNCMPT00507 RNCMPT00508 RNCMPT00509 RNCMPT00510 RNCMPT00511 RNCMPT00513 RNCMPT00514 RNCMPT00615 RNCMPT00623 RNCMPT00630 RNCMPT00676 RNCMPT00677 RNCMPT00678 RNCMPT00681 RNCMPT00683 RNCMPT00684 RNCMPT00685 RNCMPT00688 RNCMPT00689 RNCMPT00698 RNCMPT00713 RNCMPT00714 RNCMPT00716 RNCMPT00720 RNCMPT00723 RNCMPT00735 RNCMPT00741 RNCMPT00749 RNCMPT00757 RNCMPT00758 RNCMPT00760\n",
    "\n",
    "# T37201 T33328 T43095 T43077 T37262 T37172 T42977 T46227 T37266 T37258 T37267 T37238 T37187 T37210 T42997 T42995 T37165 T48473 T37185 T37277 T37213 T37204 T39282 T37283 T37251 T37272 T44425 T43050 T43008 T37174 T43087 T37219 T43099 T42990 T37177 T37299 T37236 T37168 T43000 T33331 T37341 T42988 T43009 T37259 T42986 T43194 T44422 T44427 T43097 T44429 T43040 T43004 T42987 T42996 T37170 T43014 T44426 T42989 T43037 T43019 T43084 T42991 T43069 T37205 T37245 T37287 T37281 T37171 T37180 T37199 T37222 T37244 T37246 T37226 T37298 T37324 T37178 T37319 T37256 T37228 T37271 T46233 T45868 T42981 T48474 T46225 T37221 T37269 T47360 T47376 T47405 T47407 T30410 T50126 T45229 T47656 T47668 T48477 T47689 T47034 T48164 T53390 T48472 T48476 T58636 T47134 T47397 T48237 T51619 T45194 T39359 T42083 T36104 T49508 T47635 T48475 T47690 T47050 T47052 T33333 T48065 T48115 T56083 T56086 T37164 T48469 T42999 T37716 T42887 T42914 T42910 T36184 T32682 T42067 T37595 T42847 T37624 T080691 T111509 T073162 T111417 T129636 T073165 T079123 T129761 T073174 T129709 T129642 T129797 T129786 T129692 T129707 T129767 T129768 T105245 T069727 T129701 T082561 T110902 T075869 T129682 T111426 T111427 T115662 T079071 T088427 T101422 T073133 T082501 T129781 T129794 T073119 T080824 T129619 T129616 T129748 T129646 T115700 T115639 T069669 T105602 T115653 T075921 T075947 T115709 T115646 T089448 T090360 T089444 T097553 T117459 T081135 T129595 T079230 T109640 T131099 T079046 T089305 T129791 T097567 T097570 T090375 T097584 T097585 T097603 T097581 T109637 T112011 T129674 T111999 T083362 T083377 T081209\n",
    "\n",
    "# ##Test:\n",
    "\n",
    "# RNCMPT00762 RNCMPT00768 RNCMPT00773 RNCMPT00774 RNCMPT00776 RNCMPT00788 RNCMPT00789 RNCMPT00793 RNCMPT00794 RNCMPT00799 RNCMPT00800 RNCMPT00801 RNCMPT00805 RNCMPT00808 RNCMPT00813 RNCMPT00814 RNCMPT00816 RNCMPT00818 RNCMPT00819 RNCMPT00820 RNCMPT00828 RNCMPT00830 RNCMPT00831 RNCMPT00862\n",
    "\n",
    "# T097586 T129535 T111988 T088577 T090343 T090321 T129615 T105243 T115623 T069661 T105234 T115651 T080754 T080669 T080793 T080675 T115665 T080654 T080807 T080792 T101609 T129750 T129753 T129645\n",
    "\n",
    "# #trainProts:\n",
    "\n",
    "# 4 1 19 16 9 11 5 12 17 14 3 11 2 19 23 38 10 4 8 29 7 14 23 22\n",
    "\n",
    "# #max(ident):\n",
    "\n",
    "# 0.8715 0.6615 0.8026 0.8161 0.9164 0.8593 0.7995 0.743 0.7264 0.8736 0.6975 0.7634 0.9555 0.9238 0.8614 0.8597 0.8691 0.9836 0.6873 0.9723 0.7213 0.921 0.7807 0.8647 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset: dropbox_files/Full_predictableRRM_pearson_gt3_trainset216_test24.txt 0\n",
      "0\n",
      "(24, 16382)\n",
      "(213, 16382)\n"
     ]
    }
   ],
   "source": [
    "# this file does not exist .... This should be the matching embeddings then\n",
    "\n",
    "# proteinfeatures= \"Proteinsequence_latentspacevectors.txt\"\n",
    "# P = np.genfromtxt(proteinfeatures, skip_header = 1)[:,1:]\n",
    "# P = P.T\n",
    "\n",
    "def original_script_dataset_processing(datnames=ynames, arg1=\"0(,24,48...)\", arg2=file_2):\n",
    "    numset = arg1\n",
    "    trainfile = arg2\n",
    "    \n",
    "    print( \"trainset:\", trainfile, numset)\n",
    "    tobj = open(trainfile, 'r')\n",
    "    tlines = tobj.readlines()\n",
    "    for i, tline in enumerate(tlines):\n",
    "#         print(tline[:6], tline[:6]==\"###Set\")\n",
    "#         if tline[:6]==\"###Set\":\n",
    "#             print(tline.strip().split()[-1])\n",
    "        if tline[:6]==\"###Set\" and tline.strip().split()[-1] == numset:\n",
    "            print(i)\n",
    "            trainprots = np.array([tlines[i+2].strip().split(), tlines[i+3].strip().split()]).T\n",
    "            testprots = np.array([tlines[i+5].strip().split(), tlines[i+6].strip().split()]).T\n",
    "#             break\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "                        ### sort into training and test set\n",
    "            if len(np.intersect1d(trainprots[:,0], datnames)) !=0:\n",
    "                trainprots = trainprots[:,0]\n",
    "                testprots = testprots[:,0]\n",
    "            else: \n",
    "                trainprots = trainprots[:,1]\n",
    "                testprots = testprots[:,1]\n",
    "#             print('trainprots: \\n')\n",
    "#             print(trainprots)\n",
    "#             print('testprots: \\n')\n",
    "#             print(testprots)\n",
    "            indexestrain = []\n",
    "            indexestest = []\n",
    "            for i, ina in enumerate(datnames):\n",
    "                if ina in trainprots:\n",
    "                    indexestrain.append(i)\n",
    "                elif ina in testprots:\n",
    "                    indexestest.append(i)\n",
    "            indexestrain = np.array(indexestrain)\n",
    "            indexestest = np.array(indexestest)\n",
    "#             print('indexestrain: \\n')\n",
    "#             print(indexestrain)\n",
    "            Ytrain = Y[indexestrain]\n",
    "#             Ptrain = P[indexestrain]\n",
    "            Ytest = Y[indexestest]\n",
    "#             Ptest = P[indexestest]\n",
    "            trainprots = datnames[indexestrain]\n",
    "            testprots = datnames[indexestest]\n",
    "\n",
    "            print (np.shape(Ytest)) #, np.shape(Ptest))\n",
    "            print( np.shape(Ytrain)) #, np.shape(Ptrain))\n",
    "    return Ytrain, Ytest #Ptrain, Ytest, Ptest\n",
    "\n",
    "Y_train, Y_test = original_script_dataset_processing(arg1=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### other data processing from orig file\n",
    "# --data --z Fullset_z_scores_setAB.txt ones Proteinsequence_latentspacevectors.txt\n",
    "\n",
    "# elif \"--data\" in sys.argv:\n",
    "#     #### read in files here\n",
    "dtype = '--z' #sys.argv[sys.argv.index(\"--data\")+1]\n",
    "profiles = \"dropbox_files/Fullset_z_scores_setAB.txt\" #sys.argv[sys.argv.index(\"--data\")+2]\n",
    "probefeatures = \"ones\" #sys.argv[sys.argv.index(\"--data\")+3]\n",
    "proteinfeatures = \"Proteinsequence_latentspacevectors.txt\" # sys.argv[sys.argv.index(\"--data\")+4]\n",
    "#### get feature matrices and names\n",
    "\n",
    "Y = np.genfromtxt(profiles, skip_header =1)[:,1:]\n",
    "ynames = np.array(open(profiles, 'r').readline().strip().split()[1:])\n",
    "\n",
    "\n",
    "\n",
    "Y = np.nan_to_num(Y).T\n",
    "D = []\n",
    "datnames= ynames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RNCMPT00002', array([-0.72953534, -0.33444866, -0.58822697, ...,  0.12119874,\n",
       "        -0.21044336, -0.58822697]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ynames[1], Y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making dataset for easy looping\n",
    "class LowDimData(Dataset):\n",
    "    def __init__(self, train, labels, transform=None):\n",
    "        self.low_dim_embs = train\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.low_dim_embs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.low_dim_embs[idx]\n",
    "        lab = self.labels[idx]\n",
    "        \n",
    "        sample = (emb, lab)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "# for x in input_data:\n",
    "#     print(x[1])\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # affinity regression:\n",
    "\n",
    "# # RWA^T = Y\n",
    "# # R:  low-level embeddings\n",
    "# # W: weight matrix to learn\n",
    "# # A: RNA/protein sequences to match\n",
    "# # Y: known matches\n",
    "## following class returns LHS \n",
    "\n",
    "class AfinityRegression(nn.Module):\n",
    "    def __init__(self, emb_dim, rna_dim):\n",
    "        super(AfinityRegression, self).__init__()\n",
    "        # want to learn weights after applying to embedding layer (First do RW , then RW A^T)\n",
    "        self.lin1 = nn.Linear(emb_dim, rna_dim)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        return None\n",
    "    \n",
    "    def forward(self, batch_size, embs, rna_samples):\n",
    "        self.batch_size = batch_size\n",
    "        x = self.lin1(embs)\n",
    "        x = torch.matmul(x, rna_samples)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions and current dummy values\n",
    "# samples = batch_size                              777\n",
    "# embedding_dim = learned embedding dim (emb_dim)   10\n",
    "# hidden1 = embedding_dim                           10\n",
    "# hidden2 = dimension of rna samples (rna_dim)      2555\n",
    "# rna_samples = # of RNA samples we're looking at   30\n",
    "\n",
    "\n",
    "def training_loop(batch_size, num_epochs, model, optim, data_iter, rna_options):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    total_batches = int(len(data_iter) / batch_size)\n",
    "    poss_matches = Variable(rna_options)\n",
    "    while epoch <= num_epochs:\n",
    "        for x in data_iter:\n",
    "            learned_embs = Variable(x[0])\n",
    "            known_matches = Variable(x[1])\n",
    "            \n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "\n",
    "            outs = test_model.forward(3, learned_embs, poss_matches)\n",
    "            # print(outs.size())\n",
    "            new_mat = known_matches - outs\n",
    "            loss = torch.bmm(new_mat.view(new_mat.size()[0], 1, new_mat.size()[1]),\n",
    "                               new_mat.view(new_mat.size()[0], new_mat.size()[1], 1)\n",
    "                              )\n",
    "            loss = torch.sqrt(loss)\n",
    "            loss = torch.div(torch.sum(loss.view(-1)), batch_size)\n",
    "            losses.append(loss[0].data.numpy()) \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "\n",
    "        epoch += 1\n",
    "#         if epoch % 2 == 0:\n",
    "        print( \"Epoch:\", (epoch), \"Avg Loss:\", np.mean(losses)/(total_batches*epoch) )\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Avg Loss: 1.21311803018\n",
      "Epoch: 2 Avg Loss: 0.362865263416\n",
      "Epoch: 3 Avg Loss: 0.187756015408\n",
      "Epoch: 4 Avg Loss: 0.12050919379\n",
      "Epoch: 5 Avg Loss: 0.0866596098869\n",
      "Epoch: 6 Avg Loss: 0.0668009224758\n"
     ]
    }
   ],
   "source": [
    "# dummy data\n",
    "learned_embs = torch.randn((777,10))\n",
    "poss_matches = torch.randn((2555, 30))\n",
    "known_matches = torch.randn((777,30))\n",
    "\n",
    "test_model = AfinityRegression(emb_dim=10, rna_dim=2555)\n",
    "optimizer = SGD(test_model.parameters(), lr = 0.001)\n",
    "test_model.init_weights()\n",
    "# input_data = (learned_embs, known_matches)\n",
    "batch_size = 5\n",
    "num_epochs = 5\n",
    "input_data = DataLoader(LowDimData(learned_embs, known_matches), batch_size=batch_size)\n",
    "training_loop(batch_size, num_epochs, test_model, optimizer, input_data, poss_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213, 213)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying with Y_train\n",
    "yyt = np.dot(Y_train, Y_train.T)\n",
    "yyt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Avg Loss: 82070560.0\n",
      "Epoch: 2 Avg Loss: 43743908.0\n",
      "Epoch: 3 Avg Loss: 29083077.3333\n",
      "Epoch: 4 Avg Loss: 21889334.0\n",
      "Epoch: 5 Avg Loss: 17409996.8\n",
      "Epoch: 6 Avg Loss: 14608654.6667\n"
     ]
    }
   ],
   "source": [
    "# affinity regression on Y train /randomized \"embedding\"\n",
    "\n",
    "\n",
    "learned_embs = torch.randn((213,10))\n",
    "poss_matches = torch.FloatTensor(yyt) # torch.randn((2555, 30))\n",
    "known_matches = torch.FloatTensor(yyt) # torch.randn((213,30))\n",
    "\n",
    "test_model = AfinityRegression(emb_dim=10, rna_dim=213)\n",
    "optimizer = SGD(test_model.parameters(), lr = 0.001)\n",
    "test_model.init_weights()\n",
    "# input_data = (learned_embs, known_matches)\n",
    "batch_size = 5\n",
    "num_epochs = 5\n",
    "input_data = DataLoader(LowDimData(learned_embs, known_matches), batch_size=batch_size)\n",
    "training_loop(batch_size, num_epochs, test_model, optimizer, input_data, poss_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "wclf.fit(UVdp,Yv)\n",
    "UVdp = \n",
    "Yv = \n",
    "\n",
    "Y_ = np.dot(Ytrain, Ytrain.T)\n",
    "Yv = Y_.ravel()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "if '--similarity' in sys.argv:\n",
    "    print \"kronecker...\"\n",
    "    #UVdp1 = np.kron(Vp.T, Ud)\n",
    "    d1a = np.shape(Ud)[0]\n",
    "    d1b = np.shape(Vpt)[0]\n",
    "    d2a = np.shape(Ud)[1]\n",
    "    d2b = np.shape(Vpt)[1]\n",
    "    UVdp = np.zeros((d1a*d1b,d2a*d2b))\n",
    "    for i in range(np.shape(Vpt)[0]):\n",
    "        for j in range(np.shape(Vpt)[1]):\n",
    "            UVdp[i*d1a:(i+1)*d1a, j*d2a:(j+1)*d2a] = Vpt[i,j] * Ud\n",
    "    \n",
    "    \n",
    "    \n",
    "Fullset_z_scores_setAB.txt: profiles : these are \n",
    "ones : probefeatures : \n",
    "Proteinsequence_latentspacevectors.txt : proteinfeatures : Y : \n",
    "    if probefeatures == 'ones':\n",
    "        D_ = Ytrain\n",
    "\n",
    "        \n",
    "profiles = sys.argv[sys.argv.index(\"--data\")+2] Fullset_z_scores_setAB.txt\n",
    "probefeatures = sys.argv[sys.argv.index(\"--data\")+3] D_ = Ytrain\n",
    "proteinfeatures = sys.argv[sys.argv.index(\"--data\")+4] Proteinsequence_latentspacevectors.txt        \n",
    "\n",
    "# --data --z Fullset_z_scores_setAB.txt ones Proteinsequence_latentspacevectors.txt        \n",
    "\n",
    "\n",
    "So labels = profiles?\n",
    "\n",
    "    if dtype == '--z':\n",
    "        Y = np.genfromtxt(profiles, skip_header =1)[:,1:]\n",
    "        ynames = np.array(open(profiles, 'r').readline().strip().split()[1:])\n",
    "        Y = np.nan_to_num(Y).T\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
